# AI System Configuration - User-Friendly Edition
# This file controls how the AI responds to user questions
# Make changes here to adjust response quality, speed, and detail level

# ============================================================================
# RESPONSE MODES - Control how detailed and thorough AI responses are
# ============================================================================

response_modes:
  # Quick Mode - Fast, concise answers
  quick:
    name: "Quick Answer"
    description: "Fast responses using available SOPs as needed"
    llm: "gpt-4o-mini"
    max_response_words: 500
    temperature: 0.2
    chain_of_thought: false
    
  # Standard Mode - Balanced detail level (Default)
  standard:
    name: "Standard Answer"
    description: "Balanced responses that analyze all relevant SOPs"
    llm: "gpt-4o"
    max_response_words: 1000
    temperature: 0.4
    chain_of_thought: false
    
  # Comprehensive Mode - Deep analysis with reasoning
  comprehensive:
    name: "Comprehensive Answer"
    description: "Detailed analysis with step-by-step reasoning using all SOPs"
    max_response_words: 2000
    # LLM and temperature controlled via Chain of Thought stages
    chain_of_thought: true
    reasoning_steps: ["analyze_query", "research_sops", "synthesize_answer", "validate_response"]
    refinement:
      enabled: true
      max_iterations: 3
      refinement_steps: ["research_sops", "synthesize_answer"]
      confidence_threshold: 0.8
      improvement_threshold: 0.1
      timeout_per_iteration_ms: 120000

# Default response mode for new conversations
default_response_mode: "standard"

# ============================================================================
# CHAIN-OF-THOUGHT REASONING - For comprehensive responses
# ============================================================================

chain_of_thought:
  enabled: true
  stages:
    analyze_query:
      description: "Understand what the user is really asking"
      llm: "gpt-4o"
      temperature: 0.3
      
    research_sops:
      description: "Deep analysis of each relevant SOP"
      llm: "gpt-4o"
      temperature: 0.2
      
    synthesize_answer:
      description: "Combine insights from multiple SOPs"
      llm: "gpt-4o"
      temperature: 0.4
      
    validate_response:
      description: "Check completeness and accuracy"
      llm: "gpt-4o"
      temperature: 0.2

# CUSTOMIZING REASONING STEPS:
# You can customize which steps are used by modifying the reasoning_steps array in response modes.
# Available steps: "analyze_query", "research_sops", "synthesize_answer", "validate_response"
# 
# Examples:
# - Quick reasoning: ["research_sops", "synthesize_answer"] 
# - Analysis focus: ["analyze_query", "research_sops", "validate_response"]
# - Full reasoning: ["analyze_query", "research_sops", "synthesize_answer", "validate_response"] (default)
#
# ITERATIVE REFINEMENT:
# For comprehensive mode, you can enable iterative refinement to improve answers through multiple passes:
# - enabled: true/false - Enable iterative refinement
# - max_iterations: Number of additional refinement passes (beyond initial reasoning)
# - refinement_steps: Which steps to repeat during refinement (typically ["research_sops", "synthesize_answer"])
# - confidence_threshold: Stop refining when confidence reaches this level (0.0-1.0)
# - improvement_threshold: Only continue if confidence improves by at least this much (0.0-1.0)
# - timeout_per_iteration_ms: Max time per refinement iteration in milliseconds

# ============================================================================
# SOP DIRECTORY MANAGEMENT - Auto-updating SOP overview
# ============================================================================

sop_directory:
  # Automatically update the SOP directory when SOPs change
  auto_generate: true
  
  # File location for the SOP directory
  directory_file: "sop-directory.md"
  
  # What to include in the directory
  include_topics: true
  include_relationships: true
  include_summaries: true
  include_keywords: false  # As requested
  
  # Allow editing in admin center
  editable_in_admin: true
  
  # Update triggers
  update_on_sop_create: true
  update_on_sop_edit: true
  update_on_sop_delete: true

# ============================================================================
# CONTEXT MANAGEMENT - Handle conversation history and token limits
# ============================================================================

context_management:
  # Conversation history settings
  conversation_history:
    max_messages: 6  # Keep last 6 message pairs in context
    summarize_older: true  # Summarize older messages when needed
    summary_max_words: 100  # Max words for conversation summaries
    
  # Token limit management
  token_limits:
    soft_limit: 6000  # Start optimizing context at this point
    hard_limit: 8000  # Maximum tokens allowed
    warning_threshold: 7000  # Warn user when approaching limit
    
  # What to do when hitting context limits
  overflow_strategy:
    method: "chain_of_thought_with_summarization"
    priority_order: ["current_query", "sop_content", "conversation_summary"]
    
  # Progressive context expansion for comprehensive mode
  progressive_expansion:
    enabled: true
    initial_sops: 3
    max_expansion_sops: 7
    expansion_threshold: 0.7  # Confidence threshold for expanding

# ============================================================================
# FEEDBACK AND IMPROVEMENT - User feedback handling
# ============================================================================

feedback_system:
  # Auto-trigger comprehensive mode on negative feedback
  auto_comprehensive_on_thumbs_down: true
  
  # Confidence thresholds
  confidence_thresholds:
    high: 0.8  # High confidence responses
    medium: 0.6  # Medium confidence - might need improvement
    low: 0.4  # Low confidence - suggest comprehensive mode
    
  # Auto-suggest comprehensive mode for low confidence
  auto_suggest_comprehensive: true


# ============================================================================
# SOP SELECTION AND RETRIEVAL
# ============================================================================

sop_selection:
  # How to select SOPs for a query
  llm: "gpt-4o"
  selection_method: "semantic_similarity"
  min_confidence: 0.4
  prefer_recent_sops: false
  
  # Multi-SOP handling
  multi_sop:
    enabled: true
    combination_strategy: "semantic_weighted"
    handle_duplicates: "intelligent_merge"
    detect_relationships: true

# ============================================================================
# SYSTEM PROMPTS - Core instructions for the AI
# ============================================================================

prompts:
  system_base: |
    You are an expert PMO consultant with 15+ years of experience. You help users with project management questions using company SOPs and general PM expertise. Always provide practical, actionable guidance.

  sop_selection: |
    Analyze the user's question and identify the most relevant SOPs. Consider the query intent, required depth of response, and cross-topic relationships. Respond with JSON only.

  chain_of_thought_analyze: |
    Carefully analyze this project management question to understand what the user really needs to know. Consider the context, implied requirements, and potential follow-up questions.

  chain_of_thought_research: |
    For each relevant SOP, extract the key information that directly addresses the user's question. Focus on actionable steps, deliverables, and practical guidance.

  chain_of_thought_synthesize: |
    Combine insights from multiple SOPs into a coherent, comprehensive answer. Show how different procedures work together and highlight any dependencies or relationships.

  chain_of_thought_validate: |
    Review the response for completeness, accuracy, and usefulness. Ensure all aspects of the user's question have been addressed and the guidance is practical.

  general_knowledge_system: |
    You are an expert PMO consultant with 15+ years of experience. When users ask questions that aren't directly covered by available SOPs, provide helpful project management guidance based on industry best practices. Always respond in JSON format with the following structure:
    {
      "answer": "Your comprehensive answer here",
      "methodologies": ["List of relevant methodologies"],
      "recommendedTools": ["List of recommended tools"],
      "bestPractices": ["List of best practices"]
    }

  general_knowledge_user: |
    {{userQuery}}

  sop_generation_system: |
    You are an expert PMO consultant. Answer user questions based on the provided SOP content. Be comprehensive and practical in your responses. Always cite specific sections from the SOPs when relevant.

  sop_generation_user: |
    Based on the following SOP content, please answer this question: {{userQuery}}

    {{#if primarySOP}}
    Primary SOP: {{primarySOP.title}}
    {{primarySOP.content}}
    {{/if}}

    {{#if supportingSOPs}}
    Supporting SOPs:
    {{#each supportingSOPs}}
    - {{this.title}}
    {{this.content}}
    {{/each}}
    {{/if}}

    {{#if hasConversationHistory}}
    {{conversationHistory}}
    {{/if}}

# ============================================================================
# FEATURE FLAGS - Enable/disable specific features
# ============================================================================

features:
  enable_chain_of_thought: true
  enable_sop_directory: true
  enable_response_modes: true
  enable_context_expansion: true
  enable_auto_comprehensive: true
  enable_conversation_summaries: true

# ============================================================================
# DEBUGGING AND MONITORING
# ============================================================================

debug:
  log_response_modes: true
  log_context_usage: true
  log_chain_of_thought_steps: true
  log_sop_selection_reasoning: true
  save_comprehensive_triggers: true

# ============================================================================
# ENVIRONMENT-SPECIFIC OVERRIDES
# ============================================================================

environments:
  development:
    debug:
      log_response_modes: true
      log_context_usage: true
    response_modes:
      quick:
        temperature: 0.3  # Slightly higher for testing
        
  production:
    debug:
      log_response_modes: false
      log_context_usage: false
    context_management:
      token_limits:
        hard_limit: 7500  # Slightly more conservative
        
  testing:
    response_modes:
      quick:
        llm: "gpt-4o-mini"  # Use cheaper model for tests
      standard:
        llm: "gpt-4o-mini"
      comprehensive:
        max_response_words: 500  # Shorter for faster tests
    sop_selection:
      llm: "gpt-4o-mini"
    chain_of_thought:
      stages:
        analyze_query:
          llm: "gpt-4o-mini"
        research_sops:
          llm: "gpt-4o-mini"
        synthesize_answer:
          llm: "gpt-4o-mini"
        validate_response:
          llm: "gpt-4o-mini"