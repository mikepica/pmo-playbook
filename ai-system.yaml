# AI System Configuration - Technical Settings
# 
# This file contains all technical configurations, models, and system behavior settings.
# It works together with ai-prompts.yaml to provide complete AI configuration.
# 
# FILE RELATIONSHIP:
# - ai-system.yaml (this file): Technical settings, models, and system behavior
# - ai-prompts.yaml: All user-customizable text and prompts
# 
# USAGE:
# - Configure LLMs, temperatures, token limits, and system behavior
# - Reference prompts from ai-prompts.yaml using prompt_config section
# - Override technical settings per environment (dev/prod/testing)
# - Control features, debugging, and performance settings
# 
# LOADING CONFIGURATION:
# Your application should:
# 1. Load this file for technical configuration
# 2. Load ai-prompts.yaml and merge the active prompt set
# 3. Use prompts_config.active_prompt_set to determine which prompts to use
# 
# Example pseudocode:
#   system_config = loadYAML("ai-system.yaml")
#   prompts_config = loadYAML("ai-prompts.yaml")
#   active_set = system_config.prompts_config.active_prompt_set
#   active_prompts = prompts_config.prompt_sets[active_set]
#   final_config = merge(system_config, active_prompts)
# 
# ENVIRONMENT SWITCHING:
# Different environments can use different prompt sets:
# - development: Use "default" prompts with debug logging
# - production: Use "default" prompts with minimal logging  
# - testing: Use "technical" prompts with cheaper models

# ============================================================================
# PROMPTS FILE REFERENCE
# ============================================================================

# Reference to prompts configuration file
prompts_config:
  file: "ai-prompts.yaml"
  active_prompt_set: "default"  # Can be overridden to switch prompt sets

# ============================================================================
# RESPONSE MODES - Technical configuration only
# ============================================================================

response_modes:
  # Quick Mode - Technical settings
  quick:
    llm: "gpt-4o-mini"
    max_response_words: 500
    temperature: 0.2
    chain_of_thought: false
    
  # Standard Mode - Technical settings (Default)
  standard:
    llm: "gpt-4o"
    max_response_words: 1000
    temperature: 0.4
    chain_of_thought: false
    
  # Comprehensive Mode - Technical settings
  comprehensive:
    max_response_words: 2000
    # LLM and temperature controlled via Chain of Thought stages
    chain_of_thought: true
    reasoning_steps: ["analyze_query", "research_sops", "synthesize_answer", "validate_response"]
    refinement:
      enabled: true
      max_iterations: 3
      refinement_steps: ["research_sops", "synthesize_answer"]
      confidence_threshold: 0.8
      improvement_threshold: 0.1
      timeout_per_iteration_ms: 120000

# Default response mode for new conversations
default_response_mode: "standard"

# ============================================================================
# DEFAULT/FALLBACK MODELS - Used when specific configs are missing
# ============================================================================

defaults:
  # Primary model for most operations
  primary_model: "gpt-4o"
  # Lightweight model for simple tasks
  lightweight_model: "gpt-4o-mini" 
  # Default temperature for creative responses
  creative_temperature: 0.6
  # Default temperature for analytical responses
  analytical_temperature: 0.3

# ============================================================================
# CHAIN-OF-THOUGHT REASONING - Technical configuration
# ============================================================================

chain_of_thought:
  enabled: true
  stages:
    analyze_query:
      llm: "gpt-4o"
      temperature: 0.3
      
    research_sops:
      llm: "gpt-4o"
      temperature: 0.2
      
    synthesize_answer:
      llm: "gpt-4o"
      temperature: 0.4
      
    validate_response:
      llm: "gpt-4o"
      temperature: 0.2

# CUSTOMIZING REASONING STEPS:
# You can customize which steps are used by modifying the reasoning_steps array in response modes.
# Available steps: "analyze_query", "research_sops", "synthesize_answer", "validate_response"
# 
# Examples:
# - Quick reasoning: ["research_sops", "synthesize_answer"] 
# - Analysis focus: ["analyze_query", "research_sops", "validate_response"]
# - Full reasoning: ["analyze_query", "research_sops", "synthesize_answer", "validate_response"] (default)
#
# ITERATIVE REFINEMENT:
# For comprehensive mode, you can enable iterative refinement to improve answers through multiple passes:
# - enabled: true/false - Enable iterative refinement
# - max_iterations: Number of additional refinement passes (beyond initial reasoning)
# - refinement_steps: Which steps to repeat during refinement (typically ["research_sops", "synthesize_answer"])
# - confidence_threshold: Stop refining when confidence reaches this level (0.0-1.0)
# - improvement_threshold: Only continue if confidence improves by at least this much (0.0-1.0)
# - timeout_per_iteration_ms: Max time per refinement iteration in milliseconds

# ============================================================================
# SOP DIRECTORY MANAGEMENT - Auto-updating SOP overview
# ============================================================================

sop_directory:
  # Automatically update the SOP directory when SOPs change
  auto_generate: true
  
  # File location for the SOP directory
  directory_file: "sop-directory.md"
  
  # What to include in the directory
  include_topics: true
  include_relationships: true
  include_summaries: true
  include_keywords: false  # As requested
  
  # Allow editing in admin center
  editable_in_admin: true
  
  # Update triggers
  update_on_sop_create: true
  update_on_sop_edit: true
  update_on_sop_delete: true

# ============================================================================
# CONTEXT MANAGEMENT - Handle conversation history and token limits
# ============================================================================

context_management:
  # Conversation history settings
  conversation_history:
    max_messages: 6  # Keep last 6 message pairs in context
    summarize_older: true  # Summarize older messages when needed
    summary_max_words: 100  # Max words for conversation summaries
    
  # Token limit management
  token_limits:
    soft_limit: 6000  # Start optimizing context at this point
    hard_limit: 8000  # Maximum tokens allowed
    warning_threshold: 7000  # Warn user when approaching limit
    
  # What to do when hitting context limits
  overflow_strategy:
    method: "chain_of_thought_with_summarization"
    priority_order: ["current_query", "sop_content", "conversation_summary"]
    
  # Progressive context expansion for comprehensive mode
  progressive_expansion:
    enabled: true
    initial_sops: 3
    max_expansion_sops: 7
    expansion_threshold: 0.7  # Confidence threshold for expanding

# ============================================================================
# FEEDBACK AND IMPROVEMENT - User feedback handling
# ============================================================================

feedback_system:
  # Auto-trigger comprehensive mode on negative feedback
  auto_comprehensive_on_thumbs_down: true
  
  # Confidence thresholds
  confidence_thresholds:
    high: 0.8  # High confidence responses
    medium: 0.6  # Medium confidence - might need improvement
    low: 0.4  # Low confidence - suggest comprehensive mode
    
  # Auto-suggest comprehensive mode for low confidence
  auto_suggest_comprehensive: true

# ============================================================================
# SOP SELECTION AND RETRIEVAL
# ============================================================================

sop_selection:
  # How to select SOPs for a query
  llm: "gpt-4o"
  selection_method: "semantic_similarity"
  min_confidence: 0.4
  prefer_recent_sops: false
  
  # Multi-SOP handling
  multi_sop:
    enabled: true
    combination_strategy: "semantic_weighted"
    handle_duplicates: "intelligent_merge"
    detect_relationships: true

# ============================================================================
# SESSION AND SUMMARY MANAGEMENT
# ============================================================================

session_management:
  # Model for generating session summaries
  summary_model: "gpt-4o-mini"
  summary_temperature: 0.3
  summary_max_tokens: 20

# ============================================================================
# FEATURE FLAGS - Enable/disable specific features
# ============================================================================

features:
  enable_chain_of_thought: true
  enable_sop_directory: true
  enable_response_modes: true
  enable_context_expansion: true
  enable_auto_comprehensive: true
  enable_conversation_summaries: true

# ============================================================================
# DEBUGGING AND MONITORING
# ============================================================================

debug:
  log_response_modes: true
  log_context_usage: true
  log_chain_of_thought_steps: true
  log_sop_selection_reasoning: true
  save_comprehensive_triggers: true

# ============================================================================
# ENVIRONMENT-SPECIFIC OVERRIDES
# ============================================================================

environments:
  development:
    debug:
      log_response_modes: true
      log_context_usage: true
    response_modes:
      quick:
        temperature: 0.3  # Slightly higher for testing
    prompts_config:
      active_prompt_set: "default"  # Can use different prompt sets per environment
        
  production:
    debug:
      log_response_modes: false
      log_context_usage: false
    context_management:
      token_limits:
        hard_limit: 7500  # Slightly more conservative
    prompts_config:
      active_prompt_set: "default"
        
  testing:
    response_modes:
      quick:
        llm: "gpt-4o-mini"  # Use cheaper model for tests
      standard:
        llm: "gpt-4o-mini"
      comprehensive:
        max_response_words: 500  # Shorter for faster tests
    sop_selection:
      llm: "gpt-4o-mini"
    chain_of_thought:
      stages:
        analyze_query:
          llm: "gpt-4o-mini"
        research_sops:
          llm: "gpt-4o-mini"
        synthesize_answer:
          llm: "gpt-4o-mini"
        validate_response:
          llm: "gpt-4o-mini"
    prompts_config:
      active_prompt_set: "technical"  # Use technical prompts for testing