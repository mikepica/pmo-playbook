# AI System Configuration - Technical Settings
# 
# This file contains all technical configurations, models, and system behavior settings.
# It works together with ai-prompts.yaml to provide complete AI configuration.
# 
# FILE RELATIONSHIP:
# - ai-system.yaml (this file): Technical settings, models, and system behavior
# - ai-prompts.yaml: All user-customizable text and prompts
# 
# USAGE:
# - Configure LLMs, temperatures, token limits, and system behavior
# - Reference prompts from ai-prompts.yaml using prompt_config section
# - Override technical settings per environment (dev/prod/testing)
# - Control features, debugging, and performance settings
# 
# LOADING CONFIGURATION:
# Your application should:
# 1. Load this file for technical configuration
# 2. Load ai-prompts.yaml and merge the active prompt set
# 3. Use prompts_config.active_prompt_set to determine which prompts to use
# 
# Example pseudocode:
#   system_config = loadYAML("ai-system.yaml")
#   prompts_config = loadYAML("ai-prompts.yaml")
#   active_set = system_config.prompts_config.active_prompt_set
#   active_prompts = prompts_config.prompt_sets[active_set]
#   final_config = merge(system_config, active_prompts)
# 
# ENVIRONMENT SWITCHING:
# Different environments can use different prompt sets:
# - development: Use "default" prompts with debug logging
# - production: Use "default" prompts with minimal logging  
# - testing: Use "technical" prompts with cheaper models

# ============================================================================
# PROMPTS FILE REFERENCE
# ============================================================================

# Reference to prompts configuration file
prompts_config:
  file: "ai-prompts.yaml"
  active_prompt_set: "default"  # Can be overridden to switch prompt sets

# ============================================================================
# UNIFIED PROCESSING CONFIGURATION
# ============================================================================

processing:
  # Primary model for all operations
  model: "gpt-4o"
  temperature: 0.3
  max_tokens: 2000
  max_response_words: 1500
  
  # XML structure processing
  xml_processing:
    enabled: true
    validate_structure: true
    
  # Response strategy based on SOP coverage
  coverage_thresholds:
    high_confidence: 0.7      # Full answer with citations
    medium_confidence: 0.4    # Answer with caveat about partial coverage
    low_confidence: 0.4       # Trigger escape hatch

# ============================================================================
# DEFAULT/FALLBACK MODELS - Used when specific configs are missing
# ============================================================================

defaults:
  # Primary model for most operations
  primary_model: "gpt-4o"
  # Lightweight model for simple tasks
  lightweight_model: "gpt-4o-mini" 
  # Default temperature for creative responses
  creative_temperature: 0.6
  # Default temperature for analytical responses
  analytical_temperature: 0.3

# ============================================================================
# ESCAPE HATCH CONFIGURATION
# ============================================================================

escape_hatch:
  # When to trigger escape hatch responses
  trigger_threshold: 0.4
  
  # Message configuration
  message_template: |
    The Playbook does not explicitly provide guidance for {topic}.
    
    {partial_info}
    
    üìù This appears to be a gap in our Playbook. Please leave feedback so we can add appropriate guidance for this topic.
  
  # Always show partial information even with low confidence
  show_partial_info: true
  
  # Request feedback on gaps
  request_feedback: true
  
  # Feedback prompt text
  feedback_prompt: "üìù Please leave feedback about this gap so we can improve our Playbook."

# CUSTOMIZING REASONING STEPS:
# You can customize which steps are used by modifying the reasoning_steps array in response modes.
# Available steps: "analyze_query", "research_sops", "synthesize_answer", "validate_response"
# 
# Examples:
# - Quick reasoning: ["research_sops", "synthesize_answer"] 
# - Analysis focus: ["analyze_query", "research_sops", "validate_response"]
# - Full reasoning: ["analyze_query", "research_sops", "synthesize_answer", "validate_response"] (default)
#
# ITERATIVE REFINEMENT:
# For comprehensive mode, you can enable iterative refinement to improve answers through multiple passes:
# - enabled: true/false - Enable iterative refinement
# - max_iterations: Number of additional refinement passes (beyond initial reasoning)
# - refinement_steps: Which steps to repeat during refinement (typically ["research_sops", "synthesize_answer"])
# - confidence_threshold: Stop refining when confidence reaches this level (0.0-1.0)
# - improvement_threshold: Only continue if confidence improves by at least this much (0.0-1.0)
# - timeout_per_iteration_ms: Max time per refinement iteration in milliseconds

# ============================================================================
# SOP DIRECTORY MANAGEMENT - Auto-updating SOP overview
# ============================================================================

sop_directory:
  # Automatically update the SOP directory when SOPs change
  auto_generate: true
  
  # File location for the SOP directory
  directory_file: "sop-directory.md"
  
  # What to include in the directory
  include_topics: true
  include_relationships: true
  include_summaries: true
  include_keywords: false  # As requested
  
  # Allow editing in admin center
  editable_in_admin: true
  
  # Update triggers
  update_on_sop_create: true
  update_on_sop_edit: true
  update_on_sop_delete: true

# ============================================================================
# CONTEXT MANAGEMENT - Handle conversation history and token limits
# ============================================================================

context_management:
  # Conversation history settings
  conversation_history:
    max_messages: 6  # Keep last 6 message pairs in context
    summarize_older: true  # Summarize older messages when needed
    summary_max_words: 100  # Max words for conversation summaries
    
  # Token limit management
  token_limits:
    soft_limit: 6000  # Start optimizing context at this point
    hard_limit: 8000  # Maximum tokens allowed
    warning_threshold: 7000  # Warn user when approaching limit
    
  # What to do when hitting context limits
  overflow_strategy:
    method: "chain_of_thought_with_summarization"
    priority_order: ["current_query", "sop_content", "conversation_summary"]
    
  # Progressive context expansion for comprehensive mode
  progressive_expansion:
    enabled: true
    initial_sops: 3
    max_expansion_sops: 7
    expansion_threshold: 0.7  # Confidence threshold for expanding

# ============================================================================
# FEEDBACK AND IMPROVEMENT - User feedback handling
# ============================================================================

feedback_system:
  # Auto-trigger comprehensive mode on negative feedback
  auto_comprehensive_on_thumbs_down: true
  
  # Confidence thresholds
  confidence_thresholds:
    high: 0.8  # High confidence responses
    medium: 0.6  # Medium confidence - might need improvement
    low: 0.4  # Low confidence - suggest comprehensive mode
    
  # Auto-suggest comprehensive mode for low confidence
  auto_suggest_comprehensive: true

# ============================================================================
# SOP SELECTION AND RETRIEVAL
# ============================================================================

sop_selection:
  # How to select SOPs for a query
  model: "gpt-4o"
  temperature: 0.2
  selection_method: "xml_structured"
  min_confidence: 0.3  # Lower threshold to catch partial matches
  max_sops: 3
  
  # Coverage assessment
  coverage_analysis:
    enabled: true
    gap_detection: true
    confidence_calibration: true

# ============================================================================
# SESSION AND SUMMARY MANAGEMENT
# ============================================================================

session_management:
  # Model for generating session summaries
  summary_model: "gpt-4o-mini"
  summary_temperature: 0.3
  summary_max_tokens: 20

# ============================================================================
# FEATURE FLAGS - Enable/disable specific features
# ============================================================================

features:
  enable_xml_processing: true
  enable_sop_directory: true
  enable_escape_hatch: true
  enable_inline_citations: true
  enable_confidence_scoring: true
  enable_coverage_analysis: true
  enable_conversation_summaries: true

# ============================================================================
# DEBUGGING AND MONITORING
# ============================================================================

debug:
  log_response_modes: true
  log_context_usage: true
  log_chain_of_thought_steps: true
  log_sop_selection_reasoning: true
  save_comprehensive_triggers: true

# ============================================================================
# ENVIRONMENT-SPECIFIC OVERRIDES
# ============================================================================

environments:
  development:
    debug:
      log_xml_processing: true
      log_context_usage: true
      log_coverage_analysis: true
    processing:
      temperature: 0.4  # Slightly higher for testing variety
    prompts_config:
      active_prompt_set: "default"
        
  production:
    debug:
      log_xml_processing: false
      log_context_usage: false
      log_coverage_analysis: false
    context_management:
      token_limits:
        hard_limit: 7500  # Slightly more conservative
    escape_hatch:
      trigger_threshold: 0.5  # Higher threshold for production
    prompts_config:
      active_prompt_set: "default"
        
  testing:
    processing:
      model: "gpt-4o-mini"  # Use cheaper model for tests
      max_response_words: 500  # Shorter for faster tests
    sop_selection:
      model: "gpt-4o-mini"
    escape_hatch:
      trigger_threshold: 0.3  # Lower threshold to test escape scenarios
    prompts_config:
      active_prompt_set: "default"